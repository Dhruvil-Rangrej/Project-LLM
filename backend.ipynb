# Run this code in Google Colab

!pip install -q gradio pymongo openai
!pip install vllm pyngrok
from pyngrok import ngrok

# Optional: Set your auth token if you have one
ngrok.set_auth_token("")

# Create public tunnel to port 8000
http_tunnel = ngrok.connect(addr="8000", proto="http")
print(f"Ngrok tunnel URL: {http_tunnel.public_url}")



from huggingface_hub import login

# Your Hugging Face token (must have access to the model)
login("")


!vllm serve Qwen/Qwen1.5-1.8B-Chat \
  --trust-remote-code \
  --swap-space=1 \
  --max-model-len=2048 \
  --gpu-memory-utilization=0.85 \
  --max-num-seqs 4 \
  --dtype half

